{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db94e3b1",
   "metadata": {},
   "source": [
    "# Data Preliminary Analysis and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5abc99",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook loads the original MNIST dataset and analyses its training and validation data distributions. It visualises sample images, computes class counts and percentages for training and validation splits, and saves sample images.\n",
    "\n",
    "It then defines three variants of MNIST for continual learning:\n",
    "\n",
    "* Permuted MNIST: Applies a random pixel permutation to each image.\n",
    "* Rotated MNIST: Rotates each image by a random angle.\n",
    "* Partitioned MNIST: Partitions dataset into distinct class pairs for each task.\n",
    "\n",
    "For each variant, the notebook shows how to load the modified dataset, update tasks, and visualise sample batches.\n",
    "\n",
    "The main components are:\n",
    "\n",
    "* MNIST class to load the original MNIST dataset\n",
    "* Analysis of training and validation splits\n",
    "* Visualisations of sample images and class distributions\n",
    "* PermutedMNIST class applying random permutations\n",
    "* RotatingMNIST class applying random rotations\n",
    "* PartitionMNIST class creating class-partitioned tasks\n",
    "* Methods to update tasks and show sample batches for each variant\n",
    "\n",
    "This provides a template for creating continual learning datasets by transforming MNIST and analysing their key properties. The visualisation code shows how to examine the dataset distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a684db",
   "metadata": {},
   "source": [
    "##  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d356141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torchvision.transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a3bc9",
   "metadata": {},
   "source": [
    "## Original MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    # Constructor of MNIST class\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "\n",
    "        # Define the root path for the MNIST data\n",
    "        data_root = \"mnist\"\n",
    "\n",
    "        # Load the MNIST dataset for training, perform transformations on the dataset\n",
    "        self.train_dataset = torchvision.datasets.MNIST(\n",
    "            data_root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Load training data in batches and shuffle it\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset, batch_size=128, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Load validation data in batches without shuffling\n",
    "        self.val_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                data_root,\n",
    "                train=False,\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "    def summary(self):\n",
    "        print(f'Train dataset size: {len(self.train_dataset)}')\n",
    "        print(f'Validation dataset size: {len(self.val_loader.dataset)}')\n",
    "        print(f'Training batches: {len(self.train_loader)}')\n",
    "        print(f'Validation batches: {len(self.val_loader)}')\n",
    "        print(f'Batch size: {self.train_loader.batch_size}')\n",
    "     \n",
    "    # Compute training and validation class counts\n",
    "    def compute_class_counts(self):\n",
    "        train_class_counts = Counter()\n",
    "        val_class_counts = Counter()\n",
    "\n",
    "        # Compute training class counts\n",
    "        for _, labels in self.train_loader:\n",
    "            train_class_counts.update(labels.numpy())\n",
    "\n",
    "        # Compute validation class counts\n",
    "        for _, labels in self.val_loader:\n",
    "            val_class_counts.update(labels.numpy())\n",
    "\n",
    "        return dict(train_class_counts), dict(val_class_counts)\n",
    "\n",
    "    \n",
    "# Initialise the MNIST dataset\n",
    "mnist = MNIST()\n",
    "\n",
    "# Show a summary of the dataset\n",
    "mnist.summary()\n",
    "\n",
    "# Compute the class counts for training and validation\n",
    "TRAIN_CLASS_COUNTS, VAL_CLASS_COUNTS = mnist.compute_class_counts()\n",
    "\n",
    "# Create a DataFrame to represent the data\n",
    "class_distribution_df = pd.DataFrame({\n",
    "    'Class Label': [str(label) for label in range(10)],\n",
    "    'Training Counts': [TRAIN_CLASS_COUNTS[i] for i in range(10)],\n",
    "    'Validation Counts': [VAL_CLASS_COUNTS[i] for i in range(10)]\n",
    "})\n",
    "\n",
    "# Print the DataFrame without the index\n",
    "print(class_distribution_df.to_string(index=False))\n",
    "\n",
    "# Load a single batch of validation images and their corresponding labels\n",
    "batch, labels = next(iter(mnist.val_loader))\n",
    "\n",
    "# Convert the tensors into an image\n",
    "torchvision.transforms.ToPILImage()(\n",
    "    # Create a grid of images from the tensors for visualisation\n",
    "    torchvision.utils.make_grid(\n",
    "        # Select the first 64 images\n",
    "        batch[:64],\n",
    "        # Normalise the images to bring all pixels in the range [0, 1]\n",
    "        normalize=True,\n",
    "        # Set padding around each image in the grid\n",
    "        padding=5,\n",
    "        # Set the padding value to 0.2\n",
    "        pad_value=0.2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set the context for plotting\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a figure with a size of 9x5 inches\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "\n",
    "# Create the subplot for the training split class counts\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_xlabel('Class Names')\n",
    "ax1.set_ylabel('Class Counts')\n",
    "ax1.set_title('Training Split Class Counts')\n",
    "ax1.bar(list(TRAIN_CLASS_COUNTS.keys()), list(TRAIN_CLASS_COUNTS.values()), tick_label=list(TRAIN_CLASS_COUNTS.keys()))\n",
    "\n",
    "# Create the subplot for the validation split class counts\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_xlabel('Class Names')\n",
    "ax2.set_ylabel('Class Counts')\n",
    "ax2.set_title('Validation Split Class Counts')\n",
    "ax2.bar(list(VAL_CLASS_COUNTS.keys()), list(VAL_CLASS_COUNTS.values()), tick_label=list(VAL_CLASS_COUNTS.keys()))\n",
    "ax2.set_ylim(0, max(VAL_CLASS_COUNTS.values()) + 100) # Adjust y-limit to include all counts\n",
    "\n",
    "# Adjust the subplot layout for better spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure to a PNG file\n",
    "plt.savefig('../figures/MNIST_class_count.png', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of training and validation images\n",
    "total_train_images = sum(TRAIN_CLASS_COUNTS.values())\n",
    "total_val_images = sum(VAL_CLASS_COUNTS.values())\n",
    "\n",
    "# Calculate the percentage of each class in the training and validation sets\n",
    "train_percentages = [(count / total_train_images) * 100 for count in TRAIN_CLASS_COUNTS.values()]\n",
    "val_percentages = [(count / total_val_images) * 100 for count in VAL_CLASS_COUNTS.values()]\n",
    "\n",
    "# Combine the training and validation class percentages into a single DataFrame\n",
    "class_percentages_df = pd.DataFrame({\n",
    "    'Class': list(TRAIN_CLASS_COUNTS.keys()) + list(VAL_CLASS_COUNTS.keys()),\n",
    "    'Percentage': train_percentages + val_percentages,\n",
    "    'Dataset': ['Training'] * len(TRAIN_CLASS_COUNTS) + ['Validation'] * len(VAL_CLASS_COUNTS)\n",
    "})\n",
    "\n",
    "# Round the percentages to the first number after the decimal point\n",
    "class_percentages_df['Percentage'] = class_percentages_df['Percentage'].round(2)\n",
    "\n",
    "# Set the default seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set the context for plotting\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a figure with a size of 9x5 inches\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "\n",
    "# Create the subplot for the training and validation dataset class percentages\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "sns.barplot(x='Class', y='Percentage', hue='Dataset', data=class_percentages_df, ax=ax)\n",
    "ax.set_xlabel('Class Names')\n",
    "ax.set_ylabel('Class Percentages')\n",
    "ax.set_title('Class Percentages in Training and Validation Datasets')\n",
    "\n",
    "# Adjust the subplot layout for better spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure to a PNG file\n",
    "plt.savefig('../figures/MNIST_class_percentages.png', dpi=300)\n",
    "\n",
    "print(class_percentages_df.to_string(index=False))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(mnist_dataset, path, dataset='train'):\n",
    "    # Create the directory\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Loop through the dataset and save images\n",
    "    for i, (image, _) in enumerate(mnist_dataset):\n",
    "        image_path = os.path.join(path, f\"{dataset}_{i}.jpeg\")\n",
    "        # Convert tensor to PIL Image\n",
    "        pil_image = torchvision.transforms.ToPILImage()(image[0])\n",
    "        # Save the image\n",
    "        pil_image.save(image_path)\n",
    "\n",
    "def analyse_image_sizes(path):\n",
    "    # Get a list of the image file paths within the folder that have a .jpeg extension\n",
    "    image_paths = glob.glob(os.path.join(path, \"*.jpeg\"))\n",
    "    # Analyse the distribution of image file sizes within the folder\n",
    "    image_sizes = [os.path.getsize(image_path) for image_path in image_paths]\n",
    "    print(\"Image size statistics:\")\n",
    "    print(pd.Series(image_sizes).describe())\n",
    "\n",
    "# Initialise the MNIST dataset\n",
    "mnist = MNIST()\n",
    "\n",
    "# Define the path to save the images\n",
    "path = 'mnist_images/train'\n",
    "\n",
    "# Save the training images\n",
    "save_images(mnist.train_dataset, path, dataset='train')\n",
    "\n",
    "# Analyze the image sizes\n",
    "analyse_image_sizes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906920fd",
   "metadata": {},
   "source": [
    "## Permuted MNIST Dataset Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113a5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTPerm:\n",
    "    # Inner class permute that implements the functionality of random permutation on tensors\n",
    "    class permute(object):\n",
    "        # Constructor for the inner class permute\n",
    "        def __init__(self):\n",
    "            # Initialise the permutation to the identity permutation\n",
    "            self.perm = np.arange(784)\n",
    "\n",
    "        # Callable function to flatten the tensor, perform permutation, and reshape it\n",
    "        def __call__(self, tensor):\n",
    "            out = tensor.flatten()\n",
    "            out = out[self.perm]\n",
    "            return out.view(1, 28, 28)\n",
    "\n",
    "        # Represent the class as its name\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__\n",
    "\n",
    "    # Constructor of MNISTPerm class\n",
    "    def __init__(self, seed=0):\n",
    "        super(MNISTPerm, self).__init__()\n",
    "\n",
    "        # Define the root path for the MNIST data\n",
    "        data_root = \"mnist\"\n",
    "\n",
    "        # Create an instance of the inner class permute\n",
    "        self.permuter = self.permute()\n",
    "\n",
    "        # Initialise the random seed\n",
    "        self.seed = seed\n",
    "\n",
    "        # Load the MNIST dataset for training, perform transformations on the dataset\n",
    "        train_dataset = torchvision.datasets.MNIST(\n",
    "            data_root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    self.permuter,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Load training data in batches and shuffle it\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=128, shuffle=True\n",
    "        )\n",
    "\n",
    "        # Load validation data in batches without shuffling\n",
    "        self.val_loader = torch.utils.data.DataLoader(\n",
    "            torchvision.datasets.MNIST(\n",
    "                data_root,\n",
    "                train=False,\n",
    "                transform=torchvision.transforms.Compose(\n",
    "                    [\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                        self.permuter,\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    # Function to update the permutation for a task based on the task id and the seed\n",
    "    def update_task(self, i):\n",
    "        np.random.seed(i + self.seed)\n",
    "        self.permuter.__setattr__(\"perm\", np.random.permutation(784))\n",
    "\n",
    "    # Function to reset the permutation to the original order\n",
    "    def unpermute(self):\n",
    "        self.permuter.__setattr__(\"perm\", np.arange(784))\n",
    "        \n",
    "# Initialise the permutated MNIST dataset\n",
    "mnist = MNISTPerm()\n",
    "\n",
    "# Remove permutation from the data\n",
    "mnist.unpermute()\n",
    "\n",
    "# Load a single batch of validation images and their corresponding labels\n",
    "batch, labels = next(iter(mnist.val_loader))\n",
    "\n",
    "# Update the task\n",
    "mnist.update_task(3)\n",
    "\n",
    "# Load a batch of validation images and labels after the task update\n",
    "task0, labels = next(iter(mnist.val_loader))\n",
    "\n",
    "# Convert the concatenated tensors into an image\n",
    "torchvision.transforms.ToPILImage()(\n",
    "    # Create a grid of images from the tensors for visualisation\n",
    "    torchvision.utils.make_grid(\n",
    "        # Concatenate the original batch of images with the task 0 batch along the last dimension\n",
    "        # and select the first 64 images\n",
    "        torch.cat([batch, task0], dim=-1)[:64],\n",
    "        # Normalise the images to bring all pixels in the range [0, 1]\n",
    "        normalize=True,\n",
    "        # Set padding around each image in the grid\n",
    "        padding=5,\n",
    "        # Set the padding value to 0.2\n",
    "        pad_value=0.2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b5816",
   "metadata": {},
   "source": [
    "## Rotated MNIST Dataset Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotate(object):\n",
    "    # Defines a callable object for rotating an image.\n",
    "    def __init__(self, angle=90):\n",
    "        # Initialise with a default rotation angle of 90 degrees\n",
    "        self.angle = angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Rotate the given image by self.angle degrees.\n",
    "        out = transforms.functional.rotate(img, self.angle)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Return a string representation of the Rotate class\n",
    "        return self.__class__.__name__ + '(angle={})'.format(self.angle)\n",
    "\n",
    "\n",
    "class RotatingMNIST:\n",
    "    # A class for loading and rotating MNIST data\n",
    "    def __init__(self):\n",
    "        # Initialise the class, creates the data loaders for MNIST dataset\n",
    "        super(RotatingMNIST, self).__init__()\n",
    "\n",
    "        data_root = \"mnist\"\n",
    "\n",
    "        self.rotater = Rotate()\n",
    "\n",
    "        # Define the transformations applied to the training dataset\n",
    "        train_dataset = datasets.MNIST(\n",
    "            data_root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.Grayscale(3),\n",
    "                    self.rotater,\n",
    "                    transforms.Grayscale(1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Create data loaders for the training and validation datasets\n",
    "        kwargs = {}\n",
    "        self.train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=128, shuffle=True, **kwargs\n",
    "        )\n",
    "        self.val_loader = torch.utils.data.DataLoader(\n",
    "            datasets.MNIST(\n",
    "                data_root,\n",
    "                train=False,\n",
    "                transform=transforms.Compose(\n",
    "                    [\n",
    "                        transforms.Grayscale(3),\n",
    "                        self.rotater,\n",
    "                        transforms.Grayscale(1),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def update_task(self, i):\n",
    "        # Update the rotation angle attribute of the rotater instance\n",
    "        self.rotater.__setattr__(\"angle\", random.randint(0, 360))\n",
    "        \n",
    "# Initialise a rotating MNIST dataset\n",
    "mnist = RotatingMNIST()\n",
    "\n",
    "# Update the task \n",
    "mnist.update_task(7)\n",
    "\n",
    "# Load a single batch of validation images (i) and their corresponding labels (l)\n",
    "i, l = next(iter(mnist.val_loader))\n",
    "\n",
    "# Convert the batch of tensors into an image\n",
    "torchvision.transforms.ToPILImage()(\n",
    "    # Create a grid of images from the tensors for visualisation\n",
    "    torchvision.utils.make_grid(\n",
    "        # Since there's only one batch, we concatenate along the last dimension\n",
    "        # and select the first 64 images\n",
    "        torch.cat([i], dim=-1)[:64],\n",
    "        # Normalise the images to bring all pixels in the range [0, 1]\n",
    "        normalize=True,\n",
    "        # Set padding around each image in the grid\n",
    "        padding=5,\n",
    "        # Set the padding value to 0.2\n",
    "        pad_value=0.2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878eaae",
   "metadata": {},
   "source": [
    "## Partitioned MNIST Dataset Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be27f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to partition a dataset based on the given label pair\n",
    "def partition_dataset(dataset, label_pair):\n",
    "    newdataset = copy.copy(dataset)\n",
    "\n",
    "    # Filter the data to only include images with the specified labels\n",
    "    newdataset.data = [\n",
    "        im\n",
    "        for im, label in zip(newdataset.data, newdataset.targets)\n",
    "        if label == torch.tensor(label_pair[0]) or label == torch.tensor(label_pair[1])\n",
    "    ]\n",
    "\n",
    "    # Similarly, filter the targets to only include the specified labels\n",
    "    newdataset.targets = [\n",
    "        label\n",
    "        for label in newdataset.targets\n",
    "        if label == torch.tensor(label_pair[0]) or label == torch.tensor(label_pair[1])\n",
    "    ]\n",
    "\n",
    "    return newdataset\n",
    "\n",
    "# Class to handle the partitioned MNIST dataset\n",
    "class PartitionMNIST:\n",
    "    def __init__(self):\n",
    "        super(PartitionMNIST, self).__init__()\n",
    "        data_root = \"mnist\"\n",
    "\n",
    "        # Label combinations for the 10 tasks\n",
    "        label_pairs = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (0, 2), (1, 3), (4, 6), (5, 8), (7, 9)]\n",
    "\n",
    "        # Load the training dataset with transformations\n",
    "        train_dataset = datasets.MNIST(\n",
    "            data_root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Load the validation dataset with transformations\n",
    "        val_dataset = datasets.MNIST(\n",
    "            data_root,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Partition both training and validation dataset into 10 pairs,\n",
    "        # based on the specified label combinations\n",
    "        splits = [\n",
    "            (\n",
    "                partition_dataset(train_dataset, label_pair),\n",
    "                partition_dataset(val_dataset, label_pair),\n",
    "            )\n",
    "            for label_pair in label_pairs\n",
    "        ]\n",
    "\n",
    "        # Print the length of the data in each split\n",
    "        for i in range(10):\n",
    "            print(len(splits[i][0].data))\n",
    "            print(len(splits[i][1].data))\n",
    "            print(\"==\")\n",
    "\n",
    "        kwargs = {}\n",
    "\n",
    "        # Create data loaders for each of the splits with a batch size of 128\n",
    "        self.loaders = [\n",
    "            (\n",
    "                torch.utils.data.DataLoader(\n",
    "                    x[0], batch_size=128, shuffle=True, **kwargs\n",
    "                ),\n",
    "                torch.utils.data.DataLoader(\n",
    "                    x[1], batch_size=128, shuffle=True, **kwargs\n",
    "                ),\n",
    "            )\n",
    "            for x in splits\n",
    "        ]\n",
    "\n",
    "    # Method to update the current task. Sets the train_loader and val_loader \n",
    "    # to the loaders for the given task\n",
    "    def update_task(self, i):\n",
    "        self.train_loader = self.loaders[i][0]\n",
    "        self.val_loader = self.loaders[i][1]\n",
    "\n",
    "        \n",
    "# Initialise a PartitionMNIST dataset\n",
    "mnist = PartitionMNIST()\n",
    "\n",
    "# Update task\n",
    "mnist.update_task(9)\n",
    "\n",
    "# Load a single batch of validation images (i) and their corresponding labels (l)\n",
    "i, l = next(iter(mnist.val_loader))\n",
    "\n",
    "# Convert the batch of tensors into an image\n",
    "torchvision.transforms.ToPILImage()(\n",
    "    # Create a grid of images from the tensors for visualisation\n",
    "    torchvision.utils.make_grid(\n",
    "        # Since there's only one batch, we concatenate along the last dimension\n",
    "        # and select the first 64 images\n",
    "        torch.cat([i], dim=-1)[:64],\n",
    "        # Normalise the images to bring all pixels in the range [0, 1]\n",
    "        normalize=True,\n",
    "        # Set padding around each image in the grid\n",
    "        padding=5,\n",
    "        # Set the padding value to 0.2\n",
    "        pad_value=0.2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ee027",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "* https://github.com/pytorch\n",
    "* https://github.com/RAIVNLab/supsup\n",
    "* https://www.programcreek.com/python/example/105103/torchvision.datasets.MNIST\n",
    "* https://www.programcreek.com/python/example/104832/torchvision.transforms.Compose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
