{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd21f967",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f44b570",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook implements Experiment 3 to further evaluate our classic and novel continual learning approaches on variants of the MNIST dataset.\n",
    "\n",
    "The key components are:\n",
    "\n",
    "* **Classic Approach**:\n",
    "    * Implements classic approach training by looping through tasks, training the model and evaluating performance.\n",
    "    * Saves the trained models.\n",
    "\n",
    "* **Novel Approach**:\n",
    "    * Implements novel training using our proposed method.\n",
    "    * Loads pretrained classic models to initialise batch normalisation means.\n",
    "    * Calculates task similarities and soft parameter sharing alphas.\n",
    "    * Trains using modified parameter sharing.\n",
    "\n",
    "* **Post-Training Analysis**:\n",
    "    * Generates confusion matrices and calculates performance metrics for each task.\n",
    "    * Visualises average confusion matrix and metrics to compare classic and novel approaches.\n",
    "\n",
    "The notebook demonstrates our comprehensive experiment workflow - implementing baselines, proposing a novel method, training models, evaluating performance and comparing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647c6f5",
   "metadata": {},
   "source": [
    "##  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from utilities.eval_pred_funcs import predict\n",
    "from utilities.train_funcs import train, trainV2\n",
    "from utilities.models import MultitaskFC, MultitaskFCV2\n",
    "from utilities.data import MNISTPerm, PartitionMNIST, RotatingMNIST\n",
    "from utilities.utils import cache_masks, set_model_task, set_num_tasks_learned\n",
    "from utilities.similarity_funcs import calculate_task_similarityE2, determine_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec94d81",
   "metadata": {},
   "source": [
    "## Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f38b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for classic training over specified epochs and tasks, \n",
    "# with the hidden_size set as 300\n",
    "def training_classic_pd(epochs, num_tasks, hidden_size=300):\n",
    "    # Load the permuted MNIST dataset\n",
    "    mnist = MNISTPerm()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "\n",
    "    # Initialise a results dictionary for the permuted MNIST dataset\n",
    "    perm_results = {}\n",
    "    \n",
    "    # Dictionary to store the btach normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop over each task for training and prediction\n",
    "    for task_id in range(num_tasks):\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "\n",
    "        # Create a dictionary for each task's results\n",
    "        perm_results[task_id] = {}\n",
    "\n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                perm_predictions, perm_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                perm_results[task_id][\"perm_predictions\"] = [\n",
    "                    int(x) for x in perm_predictions\n",
    "                ]\n",
    "\n",
    "                perm_results[task_id][\"perm_labels\"] = [int(x) for x in perm_labels]\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_permuted_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "\n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in perm_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/perm_task_{key}_classic.csv\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Load the rotated MNIST dataset\n",
    "    mnist = RotatingMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "\n",
    "    # Initialise a results dictionary for the rotated MNIST dataset\n",
    "    rotate_results = {}\n",
    "    \n",
    "    # Dictionary to store the bacth normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop over each task for training and prediction\n",
    "    for task_id in range(num_tasks):\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "\n",
    "        # Create a dictionary for each task's results\n",
    "        rotate_results[task_id] = {}\n",
    "\n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                rotate_predictions, rotate_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                rotate_results[task_id][\"rotate_predictions\"] = [\n",
    "                    int(x) for x in rotate_predictions\n",
    "                ]\n",
    "\n",
    "                rotate_results[task_id][\"rotate_labels\"] = [int(x) for x in rotate_labels]\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_rotated_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "\n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in rotate_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/rotate_task_{key}_classic.csv\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Load the partitioned MNIST dataset V2\n",
    "    mnist = PartitionMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "\n",
    "    # Initialise a results dictionary for the partitioned MNIST dataset\n",
    "    part_results = {}\n",
    "    \n",
    "    # Dictionary to store the bacth normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop over each task for training and prediction\n",
    "    for task_id in range(num_tasks):\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "\n",
    "        # Create a dictionary for each task's results\n",
    "        part_results[task_id] = {}\n",
    "\n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                part_predictions, part_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                part_results[task_id][\"part_predictions\"] = [\n",
    "                    int(x) for x in part_predictions\n",
    "                ]\n",
    "\n",
    "                part_results[task_id][\"part_labels\"] = [int(x) for x in part_labels]\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_partitioned_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "\n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in part_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/part_task_{key}_classic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3ab0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the function training_classic_pd\n",
    "    training_classic_pd(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf3c83",
   "metadata": {},
   "source": [
    "## Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6976f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for novel training over specified epochs and tasks, \n",
    "# with the hidden_size set as 300\n",
    "def training_novel_pd(epochs, num_tasks, hidden_size=300):\n",
    "    \n",
    "    # Load the permuted MNIST dataset\n",
    "    mnist = MNISTPerm()\n",
    "\n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "\n",
    "    # Initialise a results dictionary for the permuted MNIST dataset\n",
    "    perm_results = {}\n",
    "        \n",
    "        \n",
    "    # Loop through all tasks to load the saved models and extract batch normalisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_permuted_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "        \n",
    "\n",
    "    # Loop over each task for training and prediction\n",
    "    for task in range(num_tasks):\n",
    "        \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas_per_layer = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layers\n",
    "        model.set_alphas(alphas_per_layer)\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "        \n",
    "        # Create a dictionary for each task's results\n",
    "        perm_results[task] = {}\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "        \n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                perm_predictions, perm_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                perm_results[task][\"perm_predictions\"] = [\n",
    "                    int(x) for x in perm_predictions\n",
    "                ]\n",
    "\n",
    "                perm_results[task][\"perm_labels\"] = [int(x) for x in perm_labels]\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "\n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in perm_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/perm_task_{key}_novel.csv\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    # Load the rotated MNIST dataset\n",
    "    mnist = RotatingMNIST()\n",
    "    \n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "    # Initialise a results dictionary for the rotated MNIST dataset\n",
    "    rotate_results = {}\n",
    "    \n",
    "    \n",
    "    # Loop through all tasks to load the saved models and extract batch normalisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_rotated_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "    \n",
    "    # Loop over the range of tasks for training and prediction\n",
    "    for task in range(num_tasks):\n",
    "        \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layer\n",
    "        model.set_alphas(alphas)\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "        \n",
    "        # Initialise a dictionary for each task's results\n",
    "        rotate_results[task] = {}\n",
    "                \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "        \n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "        \n",
    "        # Initialise the optimiser for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "        \n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "            \n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "            \n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                rotate_predictions, rotate_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                rotate_results[task][\"rotate_predictions\"] = [\n",
    "                    int(x) for x in rotate_predictions\n",
    "                ]\n",
    "\n",
    "                rotate_results[task][\"rotate_labels\"] = [int(x) for x in rotate_labels]\n",
    "        \n",
    "        # Cache the mask states of the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "        \n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "    \n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in rotate_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/rotate_task_{key}_novel.csv\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # Load the partitioned MNIST dataset\n",
    "    mnist = PartitionMNIST()\n",
    "    \n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "    # Initialise the results dictionary for the partitioned MNIST dataset\n",
    "    part_results = {}\n",
    "    \n",
    "    \n",
    "    # Loop through all tasks to load the saved models and extract the batch normaloisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        prediction_directory = os.path.join(models_directory, 'prediction')\n",
    "        file_path = os.path.join(prediction_directory, f'pd_partitioned_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "    \n",
    "    # Loop over the range of tasks for training and prediction\n",
    "    for task in range(num_tasks):\n",
    "        \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layer\n",
    "        model.set_alphas(alphas)\n",
    "        \n",
    "        # Initialise a dictionary for each task's results\n",
    "        part_results[task] = {}\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "                \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "        \n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "        \n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "        \n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "            \n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "            \n",
    "            # At the end of the last epoch, predict and save the predictions and labels for the task\n",
    "            if e == epochs - 1:\n",
    "                part_predictions, part_labels = predict(model, mnist.val_loader, e)\n",
    "\n",
    "                part_results[task][\"part_predictions\"] = [\n",
    "                    int(x) for x in part_predictions\n",
    "                ]\n",
    "\n",
    "                part_results[task][\"part_labels\"] = [int(x) for x in part_labels]\n",
    "        \n",
    "        # Cache the mask states of the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "        \n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "    \n",
    "    # Save the results of each task to a CSV file\n",
    "    for key, value in part_results.items():\n",
    "        df = pd.DataFrame(value)\n",
    "        df.to_csv(f\"outputs/prediction/part_task_{key}_novel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902cb4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Call the function training_novel_pd\n",
    "    training_novel_pd(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aed1340",
   "metadata": {},
   "source": [
    "## Post-Training Results Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcedb25",
   "metadata": {},
   "source": [
    "### Permuted MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49efdae",
   "metadata": {},
   "source": [
    "#### Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['perm_task_0_classic', 'perm_task_1_classic', 'perm_task_2_classic', 'perm_task_3_classic', 'perm_task_4_classic',\n",
    "             'perm_task_5_classic', 'perm_task_6_classic', 'perm_task_7_classic', 'perm_task_8_classic', 'perm_task_9_classic']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Sum of confusion matrices for all tasks\n",
    "sum_cm = None\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "    # Calculate the performance metrics and round to 2 decimal places\n",
    "    precision = round(precision_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['perm_labels'], data['perm_predictions']) * 100, 2)\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['perm_labels'], data['perm_predictions'])\n",
    "\n",
    "    # Add the confusion matrix to the sum\n",
    "    if sum_cm is None:\n",
    "        sum_cm = cm\n",
    "    else:\n",
    "        sum_cm += cm\n",
    "\n",
    "# Average the sum of confusion matrices\n",
    "average_cm = sum_cm / len(file_names)\n",
    "\n",
    "# Create a DataFrame from the average confusion matrix\n",
    "average_cm_df = pd.DataFrame(average_cm)\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "# Create a heatmap of the average confusion matrix using seaborn\n",
    "sns.heatmap(average_cm_df, annot=True, fmt='g')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Average Confusion Matrix Across All Tasks - MNIST Permuted (Classic)')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/perm_average_confusion_matrix_classic.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics and round to 3 decimal places\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c60f9",
   "metadata": {},
   "source": [
    "#### Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aef10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['perm_task_0_novel', 'perm_task_1_novel', 'perm_task_2_novel', 'perm_task_3_novel', 'perm_task_4_novel',\n",
    "             'perm_task_5_novel', 'perm_task_6_novel', 'perm_task_7_novel', 'perm_task_8_novel', 'perm_task_9_novel']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Sum of confusion matrices for all tasks\n",
    "sum_cm = None\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "    # Calculate the performance metrics and round to 2 decimal places\n",
    "    precision = round(precision_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['perm_labels'], data['perm_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['perm_labels'], data['perm_predictions']) * 100, 2)\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['perm_labels'], data['perm_predictions'])\n",
    "\n",
    "    # Add the confusion matrix to the sum\n",
    "    if sum_cm is None:\n",
    "        sum_cm = cm\n",
    "    else:\n",
    "        sum_cm += cm\n",
    "\n",
    "# Average the sum of confusion matrices\n",
    "average_cm = sum_cm / len(file_names)\n",
    "\n",
    "# Create a DataFrame from the average confusion matrix\n",
    "average_cm_df = pd.DataFrame(average_cm)\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "# Create a heatmap of the average confusion matrix using seaborn\n",
    "sns.heatmap(average_cm_df, annot=True, fmt='g')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Average Confusion Matrix Across All Tasks - MNIST Permuted (Novel)')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/perm_average_confusion_matrix_novel.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e4246",
   "metadata": {},
   "source": [
    "### Rotated MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f3785",
   "metadata": {},
   "source": [
    "#### Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['rotate_task_0_classic', 'rotate_task_1_classic', 'rotate_task_2_classic', 'rotate_task_3_classic', 'rotate_task_4_classic',\n",
    "             'rotate_task_5_classic', 'rotate_task_6_classic', 'rotate_task_7_classic', 'rotate_task_8_classic', 'rotate_task_9_classic']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Sum of confusion matrices for all tasks\n",
    "sum_cm = None\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "     # Calculate the performance metrics and round to 2 decimal places\n",
    "    precision = round(precision_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['rotate_labels'], data['rotate_predictions']) * 100, 2)\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['rotate_labels'], data['rotate_predictions'])\n",
    "\n",
    "    # Add the confusion matrix to the sum\n",
    "    if sum_cm is None:\n",
    "        sum_cm = cm\n",
    "    else:\n",
    "        sum_cm += cm\n",
    "\n",
    "# Average the sum of confusion matrices\n",
    "average_cm = sum_cm / len(file_names)\n",
    "\n",
    "# Create a DataFrame from the average confusion matrix\n",
    "average_cm_df = pd.DataFrame(average_cm)\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "# Create a heatmap of the average confusion matrix using seaborn\n",
    "sns.heatmap(average_cm_df, annot=True, fmt='g')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Average Confusion Matrix Across All Tasks - MNIST Rotated (Classic)')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/rotate_average_confusion_matrix_classic.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c82d63",
   "metadata": {},
   "source": [
    "#### Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d24d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['rotate_task_0_novel', 'rotate_task_1_novel', 'rotate_task_2_novel', 'rotate_task_3_novel', 'rotate_task_4_novel',\n",
    "             'rotate_task_5_novel', 'rotate_task_6_novel', 'rotate_task_7_novel', 'rotate_task_8_novel', 'rotate_task_9_novel']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Sum of confusion matrices for all tasks\n",
    "sum_cm = None\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "    # Calculate the performance metrics and round to 2 decimal places\n",
    "    precision = round(precision_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['rotate_labels'], data['rotate_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['rotate_labels'], data['rotate_predictions']) * 100, 2)\n",
    "\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['rotate_labels'], data['rotate_predictions'])\n",
    "\n",
    "    # Add the confusion matrix to the sum\n",
    "    if sum_cm is None:\n",
    "        sum_cm = cm\n",
    "    else:\n",
    "        sum_cm += cm\n",
    "\n",
    "# Average the sum of confusion matrices\n",
    "average_cm = sum_cm / len(file_names)\n",
    "\n",
    "# Create a DataFrame from the average confusion matrix\n",
    "average_cm_df = pd.DataFrame(average_cm)\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "# Create a heatmap of the average confusion matrix using seaborn\n",
    "sns.heatmap(average_cm_df, annot=True, fmt='g')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Average Confusion Matrix Across All Tasks - MNIST Rotated (Novel)')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/rotate_average_confusion_matrix_novel.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6d3d6",
   "metadata": {},
   "source": [
    "### Partitioned MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb82b7",
   "metadata": {},
   "source": [
    "#### Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c69f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['part_task_0_classic', 'part_task_1_classic', 'part_task_2_classic', 'part_task_3_classic', 'part_task_4_classic',\n",
    "             'part_task_5_classic', 'part_task_6_classic', 'part_task_7_classic', 'part_task_8_classic', 'part_task_9_classic']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Define class labels for the 10 tasks\n",
    "class_labels = [\n",
    "    (0, 1), (2, 3), (4, 5), (6, 7), (8, 9),\n",
    "    (0, 2), (1, 3), (4, 6), (5, 8), (7, 9)\n",
    "]\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a figure with a grid of subplots (5 rows and 2 columns)\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12, 30))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "    # Calculate the performance metrics and round to 3 decimal places\n",
    "    precision = round(precision_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['part_labels'], data['part_predictions']) * 100, 2)\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['part_labels'], data['part_predictions'])\n",
    "\n",
    "    # Create a DataFrame from the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=class_labels[idx], columns=class_labels[idx])\n",
    "\n",
    "    # Create a heatmap of the confusion matrix using seaborn on the specific subplot\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g', ax=axes[idx])\n",
    "\n",
    "    # Add a title to the plot\n",
    "    axes[idx].set_title(f'Confusion Matrix for {file_name}')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/part_average_confusion_matrix_classic.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadb086",
   "metadata": {},
   "source": [
    "#### Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a2747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The CSV file names\n",
    "file_names = ['part_task_0_novel', 'part_task_1_novel', 'part_task_2_novel', 'part_task_3_novel', 'part_task_4_novel',\n",
    "             'part_task_5_novel', 'part_task_6_novel', 'part_task_7_novel', 'part_task_8_novel', 'part_task_9_novel']\n",
    "\n",
    "# A dictionary to store the performance metrics for each file\n",
    "performance_metrics = {\n",
    "    'Task': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'Accuracy': []\n",
    "}\n",
    "\n",
    "# Define class labels for the 10 tasks\n",
    "class_labels = [\n",
    "    (0, 1), (2, 3), (4, 5), (6, 7), (8, 9),\n",
    "    (0, 2), (1, 3), (4, 6), (5, 8), (7, 9)\n",
    "]\n",
    "\n",
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a figure with a grid of subplots (5 rows and 2 columns)\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12, 30))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Define the folder path for the CSV files\n",
    "folder_path = 'outputs/prediction/'\n",
    "\n",
    "# Iterate over each file\n",
    "for idx, file_name in enumerate(file_names):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(folder_path + file_name + '.csv')\n",
    "\n",
    "   # Calculate the performance metrics and round to 3 decimal places\n",
    "    precision = round(precision_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    recall = round(recall_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    f1 = round(f1_score(data['part_labels'], data['part_predictions'], average='macro') * 100, 2)\n",
    "    accuracy = round(accuracy_score(data['part_labels'], data['part_predictions']) * 100, 2)\n",
    "\n",
    "    # Store the metrics\n",
    "    performance_metrics['Task'].append(file_name)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1'].append(f1)\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "\n",
    "    # Calculate the confusion matrix for the predicted and actual labels\n",
    "    cm = confusion_matrix(data['part_labels'], data['part_predictions'])\n",
    "\n",
    "    # Create a DataFrame from the confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=class_labels[idx], columns=class_labels[idx])\n",
    "\n",
    "    # Create a heatmap of the confusion matrix using seaborn on the specific subplot\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g', ax=axes[idx])\n",
    "\n",
    "    # Add a title to the plot\n",
    "    axes[idx].set_title(f'Confusion Matrix for {file_name}')\n",
    "\n",
    "# Adjust the layout of the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to the figures directory\n",
    "plt.savefig('figures/part_average_confusion_matrix_novel.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the performance metrics as a table\n",
    "metrics_df = pd.DataFrame(performance_metrics)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Calculate the average of the performance metrics\n",
    "average_metrics = {\n",
    "    'Average Precision': round(sum(performance_metrics['Precision']) / len(performance_metrics['Precision']), 2),\n",
    "    'Average Recall': round(sum(performance_metrics['Recall']) / len(performance_metrics['Recall']), 2),\n",
    "    'Average F1': round(sum(performance_metrics['F1']) / len(performance_metrics['F1']), 2),\n",
    "    'Average Accuracy': round(sum(performance_metrics['Accuracy']) / len(performance_metrics['Accuracy']), 2)\n",
    "}\n",
    "\n",
    "# Print the average metrics as a table\n",
    "average_metrics_df = pd.DataFrame(list(average_metrics.items()), columns=['Metric', 'Value'])\n",
    "print(average_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f60c3e7",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "* https://github.com/pytorch\n",
    "* https://github.com/RAIVNLab/supsup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
