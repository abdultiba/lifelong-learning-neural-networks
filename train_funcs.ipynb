{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965e6ef1",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14673c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides the code for training our models. \n",
    "\n",
    "Two training functions are defined:\n",
    "\n",
    "* **train**: Implements training for our classic approach\n",
    "* **trainV2**: Implements training for the novel multitask learning approach\n",
    "\n",
    "Both functions:\n",
    "\n",
    "* Set the model to training mode\n",
    "* Define a CrossEntropyLoss criterion\n",
    "* Loop through batches\n",
    "* Perform forward and backward passes\n",
    "* Optimise model weights\n",
    "* Track metrics like running loss and accuracy\n",
    "* Return the average loss on the dataset after an epoch\n",
    "\n",
    "In addition, the train function initiates the classic model's get_bn_means method to procure the batch normalisation means of the current task.\n",
    "\n",
    "The training functions encapsulate our models' fundamental training process, including loss computation, weight updates, and metrics tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4fd27",
   "metadata": {},
   "source": [
    "##  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04832af",
   "metadata": {},
   "source": [
    "## Training the Multitask Model (Classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dea7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, epoch, task_id):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Define the loss function (cross-entropy loss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Counter to keep track of the number of correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Counter to keep track of the total number of samples seen\n",
    "    total_seen = 0\n",
    "    \n",
    "    # Counter to accumulate the total loss during training\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate through the training data loader, displaying a progress bar\n",
    "    for i, (batch, labels) in tqdm(\n",
    "        enumerate(trainloader), ascii=True, total=len(trainloader)):\n",
    "        # Forward pass of the model to compute logits\n",
    "        logits = model(batch)\n",
    "        \n",
    "        # Compute loss using the cross-entropy criterion\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Accumulate loss for the batch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients through backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform a step of the optimiser to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log progress every 20 iterations\n",
    "        if i % 20 == 0:\n",
    "            # Get the predicted labels\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Count correct predictions\n",
    "            num_correct += (predictions == labels).float().sum()\n",
    "            \n",
    "            # Increase the total number of samples seen\n",
    "            total_seen += logits.size(0)\n",
    "            \n",
    "            # Display the current epoch, step, loss, and accuracy\n",
    "            tqdm.write(\n",
    "                (\n",
    "                    f\"e{epoch} {i+1}/{len(trainloader)}\"\n",
    "                    f\" => Loss {loss.item():0.4f}, \"\n",
    "                    f\"Acc@1 {(num_correct / total_seen):0.4f}\"\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    # Get the batch normalisation means for the task\n",
    "    model.get_bn_means(task_id)\n",
    "    \n",
    "    # Calculate average loss for the entire dataset\n",
    "    average_loss = total_loss / len(trainloader)\n",
    "    \n",
    "    # Return the average loss and the batch normalisation means for the task\n",
    "    return average_loss, model.get_bn_means(task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4c635",
   "metadata": {},
   "source": [
    "## Training the Multitask Model (Novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6269ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainV2(model, trainloader, optimizer, epoch, bn_means):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Define the loss function (cross-entropy loss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Counter to keep track of the number of correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Counter to keep track of the total number of samples seen\n",
    "    total_seen = 0\n",
    "    \n",
    "    # Counter to accumulate the total loss during training\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate through the training data loader, displaying a progress bar\n",
    "    for i, (batch, labels) in tqdm(\n",
    "        enumerate(trainloader), ascii=True, total=len(trainloader)):\n",
    "        # Forward pass of the model to compute logits\n",
    "        logits = model(batch)\n",
    "\n",
    "        # Compute loss using the cross-entropy criterion\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Accumulate loss for the batch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients through backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform a step of the optimiser to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log training progress every 20 steps\n",
    "        if i % 20 == 0:\n",
    "            # Compute predictions by taking the class with the highest logit\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            \n",
    "            # Update the number of correct predictions\n",
    "            num_correct += (predictions == labels).float().sum()\n",
    "            \n",
    "            # Update the total number of predictions\n",
    "            total_seen += logits.size(0)\n",
    "            \n",
    "            # Display the current epoch, step, loss, and accuracy\n",
    "            tqdm.write(\n",
    "                (\n",
    "                    f\"e{epoch} {i+1}/{len(trainloader)}\"\n",
    "                    f\" => Loss {loss.item():0.4f}, \"\n",
    "                    f\"Acc@1 {(num_correct / total_seen):0.4f}\"\n",
    "                ),\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "    # Calculate average loss for the entire dataset\n",
    "    average_loss = total_loss / len(trainloader)\n",
    "    \n",
    "    # Return the average loss for the task\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec116b",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "* https://github.com/pytorch\n",
    "* https://github.com/RAIVNLab/supsup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
