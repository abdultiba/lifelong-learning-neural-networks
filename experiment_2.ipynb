{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36d816b",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e93edd",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook implements Experiment 2 to evaluate classic and novel continual learning approaches on variants of the MNIST dataset. \n",
    "\n",
    "The key components are:\n",
    "\n",
    "* **Classic Approach**:\n",
    "    * Implements classic approach training by looping through tasks, training the model and evaluating performance.\n",
    "    * Calculates task similarities using the batch normalisation layer means.\n",
    "    * Plots task similarity heatmaps\n",
    "\n",
    "* **Novel Approach**:\n",
    "    * Implements novel training using our proposed method.\n",
    "    * Loads pretrained classic models to initialise task-specific batch normalisation means.\n",
    "    * Calculates task similarities and soft parameter sharing alphas.\n",
    "    * Trains using modified parameter sharing\n",
    "\n",
    "* **Post-Training Procedures**:\n",
    "    * Results are collected into DataFrames.\n",
    "    * Accuracy, training time and loss are plotted for the classic and novel approaches.\n",
    "\n",
    "The notebook demonstrates our comprehensive experiment workflow - implementing baselines, proposing a novel method, training models, evaluating performance and comparing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b10c4",
   "metadata": {},
   "source": [
    "##  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed523ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities.eval_pred_funcs import evaluate\n",
    "from utilities.train_funcs import train, trainV2\n",
    "from utilities.models import MultitaskFC, MultitaskFCV2\n",
    "from utilities.data import MNISTPerm, PartitionMNIST, RotatingMNIST\n",
    "from utilities.utils import cache_masks, set_model_task, set_num_tasks_learned\n",
    "from utilities.similarity_funcs import calculate_task_similarityE1, calculate_task_similarityE2, determine_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af2b39",
   "metadata": {},
   "source": [
    "## Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for classic training over a specified number of epochs and tasks\n",
    "# with the hidden_size set as 300\n",
    "def training_classic_ev(epochs, num_tasks, hidden_size=300):\n",
    "    # Initialise a dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Load permuted MNIST dataset\n",
    "    mnist = MNISTPerm()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "    # Initialise a results dictionary for the permuted MNIST dataset\n",
    "    results[\"perm\"] = {}\n",
    "    \n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    # Dictionary to store the btach normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop through each task for training and evaluation\n",
    "    for task_id in range(num_tasks):\n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"perm\"][task_id] = {}\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)  \n",
    "        \n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            average_loss, bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "            \n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "        \n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "        \n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "        \n",
    "        # Store the training time, accuracy and loss in the results dictionary\n",
    "        results[\"perm\"][task_id][\"time\"] = time_taken\n",
    "        results[\"perm\"][task_id][\"acc\"] = acc1\n",
    "        results[\"perm\"][task_id][\"loss\"] = average_loss\n",
    "        \n",
    "        # Append the accuracy, training time and loss to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_permuted_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "    \n",
    "    # Compute the cosine similarity matrix for tasks based on the batch normalisation means\n",
    "    similarities_matrix = calculate_task_similarityE1(bn_means_dict, num_tasks)\n",
    "        \n",
    "    # Plot the similarity matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarities_matrix, cmap='Blues', annot=True, linewidths=.5)\n",
    "    plt.title('Task Similarity Heatmap - Permuted MNIST')\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Task ID')\n",
    "    plt.savefig('figures/perm_similarity_heatmap.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and store the average accuracy, training time and loss for the permuted MNIST dataset\n",
    "    results[\"perm\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"perm\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"perm\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Load rotated MNIST dataset\n",
    "    mnist = RotatingMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "    # Initialise a results dictionary for the rotated MNIST dataset\n",
    "    results[\"rotate\"] = {}\n",
    "    \n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    # Dictionary to store the bacth normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop through each task for training and evaluation\n",
    "    for task_id in range(num_tasks):\n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"rotate\"][task_id] = {}\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)  \n",
    "        \n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            average_loss, bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "            \n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "        \n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "        \n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "        \n",
    "        # Store the training time, accuracy and loss in the results dictionary\n",
    "        results[\"rotate\"][task_id][\"time\"] = time_taken\n",
    "        results[\"rotate\"][task_id][\"acc\"] = acc1\n",
    "        results[\"rotate\"][task_id][\"loss\"] = average_loss\n",
    "        \n",
    "        # Append the accuracy, training time and loss to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_rotated_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "    \n",
    "    # Compute the cosine similarity matrix for tasks based on the batch normalisation means\n",
    "    similarities_matrix = calculate_task_similarityE1(bn_means_dict, num_tasks)\n",
    "        \n",
    "    # Plot the similarity matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarities_matrix, cmap='Blues', annot=True, linewidths=.5)\n",
    "    plt.title('Task Similarity Heatmap - Rotated MNIST')\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Task ID')\n",
    "    plt.savefig('figures/rotate_similarity_heatmap.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and store the average accuracy, training time and loss for the rotated MNIST dataset\n",
    "    results[\"rotate\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"rotate\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"rotate\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # Load partitioned MNIST dataset\n",
    "    mnist = PartitionMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFC model for the tasks\n",
    "    model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "    # Initialise a results dictionary for the partitioned MNIST dataset\n",
    "    results[\"part\"] = {}\n",
    "    \n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    # Dictionary to store the bacth normalisation means for each task\n",
    "    bn_means_dict = {}\n",
    "\n",
    "    # Loop through each task for training and evaluation\n",
    "    for task_id in range(num_tasks):\n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"part\"][task_id] = {}\n",
    "        \n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task_id}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task_id)  \n",
    "        \n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task_id)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            average_loss, bn_means = train(model, mnist.train_loader, optimizer, e, task_id)\n",
    "            \n",
    "            # Store the batch normalisation means in the dictionary for the task\n",
    "            bn_means_dict[task_id] = bn_means\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "            \n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "        \n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "        \n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "        \n",
    "        # Store the training time, accuracy and loss in the results dictionary\n",
    "        results[\"part\"][task_id][\"time\"] = time_taken\n",
    "        results[\"part\"][task_id][\"acc\"] = acc1\n",
    "        results[\"part\"][task_id][\"loss\"] = average_loss\n",
    "        \n",
    "        # Append the accuracy, training time and loss to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the current state of the masks in the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task_id + 1)\n",
    "        print()\n",
    "        \n",
    "        # Save the model for the current task\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_partitioned_model_task_{task_id}.pth')\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "     \n",
    "    # Compute the cosine similarity matrix for tasks based on the batch normalisation means\n",
    "    similarities_matrix = calculate_task_similarityE1(bn_means_dict, num_tasks)\n",
    "        \n",
    "    # Plot the similarity matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarities_matrix, cmap='Blues', annot=True, linewidths=.5)\n",
    "    plt.title('Task Similarity Heatmap - Partitioned MNIST')\n",
    "    plt.xlabel('Task ID')\n",
    "    plt.ylabel('Task ID')\n",
    "    plt.savefig('figures/part_similarity_heatmap.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and store the average accuracy and time for the partitioned MNIST dataset\n",
    "    results[\"part\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"part\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"part\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "               \n",
    "    \n",
    "    # Return the results dictionary\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d1ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of training rounds for the models\n",
    "n_training_rounds = 10\n",
    "\n",
    "# Start main program execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up lists to hold accuracy, training time, and loss results for the classic approach\n",
    "    classic_acc_arr = []\n",
    "    classic_time_arr = []\n",
    "    classic_loss_arr = []\n",
    "\n",
    "    # Define column names for the resulting DataFrame that will hold accuracy, time, and loss data\n",
    "    columns_acc = [\"permutation_acc\", \"rotation_acc\", \"partition_acc\"]\n",
    "    columns_time = [\"permutation_time\", \"rotation_time\", \"partition_time\"]\n",
    "    columns_loss = [\"permutation_loss\", \"rotation_loss\", \"partition_loss\"]\n",
    "\n",
    "    # Iterate through each training round\n",
    "    for i in range(n_training_rounds):\n",
    "        # Initialise lists for the current training round\n",
    "        classic_acc_arr.append([])\n",
    "        classic_time_arr.append([])\n",
    "        classic_loss_arr.append([])\n",
    "\n",
    "        # Get results from the training function for classic methods\n",
    "        results_classic = training_classic_ev(1, 10, 300)\n",
    "\n",
    "        # Loop through the classic approach training results and store in corresponding lists\n",
    "        for key, value in results_classic.items():\n",
    "            classic_acc_arr[i].append(value[\"average_acc\"])\n",
    "            classic_time_arr[i].append(value[\"average_time\"])\n",
    "            classic_loss_arr[i].append(value[\"average_loss\"])\n",
    "\n",
    "    # Create DataFrames from classic approach results\n",
    "    df_classic_acc = pd.DataFrame(classic_acc_arr, columns=columns_acc)\n",
    "    df_classic_time = pd.DataFrame(classic_time_arr, columns=columns_time)\n",
    "    df_classic_loss = pd.DataFrame(classic_loss_arr, columns=columns_loss)\n",
    "\n",
    "    # Join accuracy, training time, and loss DataFrames\n",
    "    df_classic = df_classic_acc.join(df_classic_time).join(df_classic_loss)\n",
    "\n",
    "    # Save resulting DataFrame to a CSV file\n",
    "    df_classic.to_csv(\"outputs/evaluation/classic_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530536b",
   "metadata": {},
   "source": [
    "## Novel Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644767ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for novel training over a specified number of epochs and tasks\n",
    "# with the hidden_size set as 300\n",
    "def training_novel_ev(epochs, num_tasks, hidden_size=300):\n",
    "    \n",
    "    # Initialise an empty dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Load the permuted MNIST dataset\n",
    "    mnist = MNISTPerm()\n",
    "\n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "    \n",
    "\n",
    "    # Initialise a results dictionary for the permuted MNIST dataset\n",
    "    results[\"perm\"] = {}\n",
    "\n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    # Loop through all tasks to load the saved models and extract batch normalisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_permuted_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "\n",
    "\n",
    "\n",
    "    # Loop over the range of tasks for training and evaluation\n",
    "    for task in range(num_tasks):\n",
    "  \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas_per_layer = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layer\n",
    "        model.set_alphas(alphas_per_layer)\n",
    "       \n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"perm\"][task] = {}\n",
    "\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            average_loss = trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "\n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "\n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "\n",
    "        # Store the training time, accuracy and loss in the results dictionary\n",
    "        results[\"perm\"][task][\"time\"] = time_taken\n",
    "        results[\"perm\"][task][\"acc\"] = acc1\n",
    "        results[\"perm\"][task][\"loss\"] = average_loss\n",
    "\n",
    "        # Append the accuracy and time taken to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the mask states of the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "    \n",
    "\n",
    "    # Calculate and store the average accuracy, training time and loss for the permuted MNIST dataset\n",
    "    results[\"perm\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"perm\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"perm\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # Load the rotated MNIST dataset\n",
    "    mnist = RotatingMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "\n",
    "    # Initialise a results dictionary for the rotated MNIST dataset\n",
    "    results[\"rotate\"] = {}\n",
    "\n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    # Loop through all tasks to load the saved models and extract batch normalisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_rotated_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "\n",
    "    # Loop over the range of tasks for training and evaluation\n",
    "    for task in range(num_tasks):\n",
    "  \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layer\n",
    "        model.set_alphas(alphas)\n",
    "       \n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"rotate\"][task] = {}\n",
    "\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the updated training function for this model\n",
    "            average_loss = trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "\n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "\n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "\n",
    "        # Store the time taken, accuracy and loss in the results dictionary\n",
    "        results[\"rotate\"][task][\"time\"] = time_taken\n",
    "        results[\"rotate\"][task][\"acc\"] = acc1\n",
    "        results[\"rotate\"][task][\"loss\"] = average_loss\n",
    "\n",
    "        # Append the accuracy, training time and loss to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the mask states of the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    # Calculate and store the average accuracy, training time and loss for the rotated MNIST dataset\n",
    "    results[\"rotate\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"rotate\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"rotate\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Load the partitioned MNIST dataset\n",
    "    mnist = PartitionMNIST()\n",
    "\n",
    "    # Initialise the MultitaskFCV2 model for the tasks\n",
    "    model = MultitaskFCV2(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "\n",
    "    # Initialise the results dictionary for the partitioned MNIST dataset\n",
    "    results[\"part\"] = {}\n",
    "\n",
    "    # Initialise lists to store accuracies, training times and losses for each task\n",
    "    accs = []\n",
    "    times = []\n",
    "    losses = []\n",
    "    \n",
    "    \n",
    "    # Loop through all tasks to load the saved models and extract the batch normaloisation means values\n",
    "    for task_id in range(num_tasks):\n",
    "        current_directory = os.getcwd()\n",
    "        models_directory = os.path.join(current_directory, 'models')\n",
    "        evaluation_directory = os.path.join(models_directory, 'evaluation')\n",
    "        file_path = os.path.join(evaluation_directory, f'ev_partitioned_model_task_{task_id}.pth')\n",
    "        classic_model = MultitaskFC(hidden_size=hidden_size, num_tasks=num_tasks)\n",
    "        classic_model.load_state_dict(torch.load(file_path), strict=False)\n",
    "        bn_mean = classic_model.get_bn_means(task_id)\n",
    "        model.bn_means[task_id] = bn_mean\n",
    "        print(f\"Task {task_id}: bn_mean =\", bn_mean)\n",
    "\n",
    "    # Loop over the range of tasks for training and evaluation\n",
    "    for task in range(num_tasks):\n",
    "  \n",
    "        print(f\"Current task: {task}\")\n",
    "        print(f\"Updated bn_means (length: {len(model.bn_means)}): {model.bn_means}\")\n",
    "        \n",
    "        # Calculate the similarities matrix based on the current state of batch normalisation means\n",
    "        similarities_matrix = calculate_task_similarityE2(model.bn_means, num_tasks)\n",
    "\n",
    "        # Calculate the alphas based on the task similarities\n",
    "        alphas = determine_alphas(similarities_matrix, task)\n",
    "\n",
    "        # Set the alphas for the multitask masked linear layer\n",
    "        model.set_alphas(alphas)\n",
    "       \n",
    "        # Initialise a dictionary for each task's results\n",
    "        results[\"part\"][task] = {}\n",
    "\n",
    "        # Log the training task number\n",
    "        print(f\"Training for task {task}\")\n",
    "        \n",
    "        # Set the current task in the model\n",
    "        set_model_task(model, task)\n",
    "\n",
    "        # Update the task in the dataset\n",
    "        mnist.update_task(task)\n",
    "\n",
    "        # Initialise the optimiser (RMSprop) for model parameters that require gradient computation\n",
    "        optimizer = optim.RMSprop(\n",
    "            [p for p in model.parameters() if p.requires_grad], lr=1e-4\n",
    "        )\n",
    "\n",
    "        # Record the start time for training\n",
    "        start = time()\n",
    "\n",
    "        # Loop over each epoch to train the model\n",
    "        for e in range(epochs):\n",
    "            # Use the training function for this model\n",
    "            average_loss = trainV2(model, mnist.train_loader, optimizer, e, bn_means=model.bn_means)\n",
    "\n",
    "            # Display validation information\n",
    "            print(\"Validation\")\n",
    "            print(\"============\")\n",
    "\n",
    "            # Evaluate the model's performance on the validation dataset\n",
    "            acc1 = evaluate(model, mnist.val_loader, e)\n",
    "\n",
    "        # Record the end time for training\n",
    "        end = time()\n",
    "\n",
    "        # Calculate the elapsed time for training\n",
    "        time_taken = end - start\n",
    "\n",
    "        # Log the time taken for training\n",
    "        print(f\"Time taken: {time_taken}\")\n",
    "        print()\n",
    "\n",
    "        # Store the training time, accuracy and loss in the results dictionary\n",
    "        results[\"part\"][task][\"time\"] = time_taken\n",
    "        results[\"part\"][task][\"acc\"] = acc1\n",
    "        results[\"part\"][task][\"loss\"] = average_loss\n",
    "\n",
    "        # Append the accuracy, training time and loss to their respective lists\n",
    "        accs.append(acc1)\n",
    "        times.append(time_taken)\n",
    "        losses.append(average_loss)\n",
    "\n",
    "        # Cache the mask states of the model\n",
    "        cache_masks(model)\n",
    "        print()\n",
    "\n",
    "        # Update the number of learned tasks in the model\n",
    "        set_num_tasks_learned(model, task + 1)\n",
    "        print()\n",
    "        \n",
    "    \n",
    "    # Calculate and store the average accuracy, training time and loss for the partitioned MNIST dataset\n",
    "    results[\"part\"][\"average_acc\"] = np.average(np.array(accs))\n",
    "    results[\"part\"][\"average_time\"] = np.average(np.array(times))\n",
    "    results[\"part\"][\"average_loss\"] = np.average(np.array(losses))\n",
    "    \n",
    "    # Return the results dictionary\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3473ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of training rounds for the models\n",
    "n_training_rounds = 10\n",
    "\n",
    "# Start main program execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up lists to hold accuracy, training time, and loss results for novel approach\n",
    "    novel_acc_arr = []\n",
    "    novel_time_arr = []\n",
    "    novel_loss_arr = []\n",
    "\n",
    "    # Define column names for the resulting DataFrame that will hold accuracy, training time, and loss data\n",
    "    columns_acc = [\"permutation_acc\", \"rotation_acc\", \"partition_acc\"]\n",
    "    columns_time = [\"permutation_time\", \"rotation_time\", \"partition_time\"]\n",
    "    columns_loss = [\"permutation_loss\", \"rotation_loss\", \"partition_loss\"]\n",
    "\n",
    "    # Iterate through each training round\n",
    "    for i in range(n_training_rounds):\n",
    "        # Initialise lists for the current training round\n",
    "        novel_acc_arr.append([])\n",
    "        novel_time_arr.append([])\n",
    "        novel_loss_arr.append([])\n",
    "\n",
    "        # Get results from the training functions for novel approach\n",
    "        results_novel = training_novel_ev(1, 10, 300)\n",
    "\n",
    "        # Loop through the novel approach training results and store in corresponding lists\n",
    "        for key, value in results_novel.items():\n",
    "            novel_acc_arr[i].append(value[\"average_acc\"])\n",
    "            novel_time_arr[i].append(value[\"average_time\"])\n",
    "            novel_loss_arr[i].append(value[\"average_loss\"])\n",
    "\n",
    "    # Create DataFrames from novel approach results\n",
    "    df_novel_acc = pd.DataFrame(novel_acc_arr, columns=columns_acc)\n",
    "    df_novel_time = pd.DataFrame(novel_time_arr, columns=columns_time)\n",
    "    df_novel_loss = pd.DataFrame(novel_loss_arr, columns=columns_loss)\n",
    "\n",
    "    # Join accuracy, training time, and loss DataFrames\n",
    "    df_novel = df_novel_acc.join(df_novel_time).join(df_novel_loss)\n",
    "\n",
    "    # Save resulting DataFrame to a CSV file\n",
    "    df_novel.to_csv(\"outputs/evaluation/novel_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25202a4",
   "metadata": {},
   "source": [
    "## Post-Training Results Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read classic and novel approaches' results from CSV files\n",
    "classic_df = pd.read_csv('outputs/evaluation/classic_results.csv')\n",
    "novel_df = pd.read_csv('outputs/evaluation/novel_results.csv')\n",
    "\n",
    "# Prefix column names with classic and novel\n",
    "classic_df.columns = ['classic_' + col if col != 'n_training_rounds' else col for col in classic_df.columns]\n",
    "novel_df.columns = ['novel_' + col if col != 'n_training_rounds' else col for col in novel_df.columns]\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = classic_df.merge(novel_df, left_on='n_training_rounds', right_on='n_training_rounds')\n",
    "\n",
    "# Save merged DataFrame to CSV\n",
    "merged_df.to_csv('outputs/evaluation/merged_results.csv', index=False)\n",
    "\n",
    "# Set the default seaborn theme\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Set the context for plotting\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "# Define a color palette\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "\n",
    "# Define line styles for the classic and novel approaches' results\n",
    "line_styles_classic = ['-']\n",
    "line_styles_novel = ['--']\n",
    "\n",
    "# Define markers for each dataset\n",
    "markers = ['o', 'x', '^']\n",
    "\n",
    "# Function to create a plot for each dataset\n",
    "def create_plot(metric, y_label, file_prefix):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle(y_label, fontsize=16, fontweight='bold')\n",
    "\n",
    "    for ax, dataset, title, marker in zip(axes, ['permutation', 'rotation', 'partition'], ['Permutation', 'Rotation', 'Partition'], markers):\n",
    "        classic_column = f'classic_{dataset}_{metric}'\n",
    "        novel_column = f'novel_{dataset}_{metric}'\n",
    "\n",
    "        # Use distinct line styles and markers for each dataset\n",
    "        ax.plot(merged_df['n_training_rounds'], merged_df[classic_column], label='Classic ' + title, linestyle=line_styles_classic[0], marker=marker, color=palette[0], linewidth=2, markersize=6)\n",
    "        ax.plot(merged_df['n_training_rounds'], merged_df[novel_column], label='Novel ' + title, linestyle=line_styles_novel[0], marker=marker, color=palette[1], linewidth=2, markersize=6)\n",
    "\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_xlabel('Training Round', fontsize=12)\n",
    "        ax.set_xticks(merged_df['n_training_rounds'])\n",
    "        ax.grid(True, linestyle='--')\n",
    "\n",
    "        # Set y-axis limits and label\n",
    "        min_val = min(merged_df[classic_column].min(), merged_df[novel_column].min())\n",
    "        max_val = max(merged_df[classic_column].max(), merged_df[novel_column].max())\n",
    "        ax.set_ylim(min_val - (max_val - min_val) * 0.05, max_val + (max_val - min_val) * 0.05)\n",
    "        if dataset == 'permutation':\n",
    "            ax.set_ylabel(y_label if metric != 'time' else 'Time (seconds)', fontsize=12)\n",
    "\n",
    "    # Custom legend outside the plot\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, fontsize=10)\n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    \n",
    "    # Save the generated figures\n",
    "    plt.savefig(f'figures/{file_prefix}_{metric}.png', bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot accuracy, training time and loss\n",
    "for metric, y_label, file_prefix in zip(['acc', 'loss', 'time'], ['Accuracy', 'Loss', 'Training Time'], ['accuracy', 'loss', 'time']):\n",
    "    create_plot(metric, y_label, file_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bab63",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "* https://github.com/pytorch\n",
    "* https://github.com/RAIVNLab/supsup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
