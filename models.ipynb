{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790d347e",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec80184e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides an implementation of subnet extraction and weight initialisation for our fully connected with masked linear layers models. \n",
    "\n",
    "The key contents are as follows:\n",
    "\n",
    "* The **GetSubnet** class is designed to extract subnets from given network layers. It provides methods to determine if a score is greater than or equal to zero, making it a binary tensor. It also ensures that the gradients remain unchanged during the backward pass.\n",
    "\n",
    "* The **mask_init** function initialises a scores tensor, which has the same shape as the weight tensor of a module. The tensor is populated with values from a uniform distribution.\n",
    "\n",
    "* The **signed_constant** function modifies the weights of a given module based on its sign and a calculated constant.\n",
    "\n",
    "* The **MultitaskMaskLinear** class is a customised masked linear layer which supports the classic model learning with unique masks for each task. The mask determines which weights are active during the forward pass.\n",
    "\n",
    "* The **MultitaskFC** class is a fully connected multitask network that utilises the specialised masked linear layers, followed by batch normalisation and ReLU activation. It has methods to retrieve the batch normalisation means and subnet masks for each task.\n",
    "\n",
    "* The **MultitaskMaskLinearV2** class is identical to MultitaskMaskLinear in functionality but developed to be part of the novel approach.\n",
    "\n",
    "* The **MultitaskFCV2** class is an upgraded version of MultitaskFC that utilises the MultitaskMaskLinearV2 layers. It introduces methods for setting the alpha values for each masked linear layer.\n",
    "\n",
    "Throughout the implementation, there is a consistent emphasis on maintaining separate masks (or subnets) for each task, ensuring that the networks can be trained for continual learning across multiple tasks without catastrophic forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a47d194",
   "metadata": {},
   "source": [
    "##  Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7945f94",
   "metadata": {},
   "source": [
    "## Implementing Subnet Extraction and Weight Initialisation for Network Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to get the subnet of a network layer\n",
    "class GetSubnet(autograd.Function):\n",
    "    # Override the static method forward in the parent class.\n",
    "    @staticmethod\n",
    "    def forward(ctx, scores):\n",
    "        # Return a binary tensor where each element is 1 if the corresponding \n",
    "        # score is greater than or equal to 0, otherwise 0\n",
    "        return (scores >= 0).float()\n",
    "\n",
    "    # Override the static method 'backward' in the parent class\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        # Return the gradient g as it is, meaning this function does not alter \n",
    "        # the gradient\n",
    "        return g\n",
    "\n",
    "# Function to initialise the scores tensor, which has the same shape as the \n",
    "# weight tensor of a given module\n",
    "def mask_init(module):\n",
    "    # Create a tensor scores of the same size as the weight tensor in the module\n",
    "    scores = torch.Tensor(module.weight.size())\n",
    "    # Initialise the scores tensor with values drawn from a uniform distribution\n",
    "    nn.init.kaiming_uniform_(scores, a=math.sqrt(5))\n",
    "    return scores\n",
    "\n",
    "# Function to adjust the weights of a module by the sign of the weights times a constant\n",
    "def signed_constant(module):\n",
    "    # Calculate the correct fan-in for the given module\n",
    "    fan = nn.init._calculate_correct_fan(module.weight, \"fan_in\")\n",
    "    # Calculate the gain for a ReLU activation function\n",
    "    gain = nn.init.calculate_gain(\"relu\")\n",
    "    # Calculate the standard deviation using the gain and the fan-in\n",
    "    std = gain / math.sqrt(fan)\n",
    "    # Update the weights of the module by multiplying the sign of the weight by the \n",
    "    # calculated standard deviation\n",
    "    module.weight.data = module.weight.data.sign() * std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b877fe0",
   "metadata": {},
   "source": [
    "## MultiTask Masked Linear Layer (Classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to create a multitask linear layer with masks\n",
    "class MultitaskMaskLinear(nn.Linear):\n",
    "    def __init__(self, *args, num_tasks=1, **kwargs):\n",
    "        # Initialise the parent class with the provided arguments and keyword arguments\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store the number of tasks\n",
    "        self.num_tasks = num_tasks\n",
    "        # Create a list of parameters for the scores, initialised with the mask_init \n",
    "        # function for each task\n",
    "        self.scores = nn.ParameterList(\n",
    "            [nn.Parameter(mask_init(self)) for _ in range(num_tasks)]\n",
    "        )\n",
    "        \n",
    "        # Disable gradients for the weight tensor\n",
    "        self.weight.requires_grad = False\n",
    "        # Adjust the weights using the signed_constant function\n",
    "        signed_constant(self)\n",
    "\n",
    "    # Define a method to cache the subnet masks with no gradient tracking\n",
    "    @torch.no_grad()\n",
    "    def cache_masks(self):\n",
    "        # Register a buffer for the stacked masks\n",
    "        self.register_buffer(\n",
    "            \"stacked\",\n",
    "            # Stack the subnets for all tasks\n",
    "            torch.stack(\n",
    "                [GetSubnet.apply(self.scores[j]) for j in range(self.num_tasks)]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Override the forward pass method of the parent class\n",
    "    def forward(self, x):\n",
    "        # If task index is less than 0, perform a superimposed forward pass\n",
    "        if self.task < 0:\n",
    "            alpha_weights = self.alphas[: self.num_tasks_learned]\n",
    "            # Create a binary index mask for tasks with non-zero alpha weights\n",
    "            idxs = (alpha_weights > 0).squeeze().view(self.num_tasks_learned)\n",
    "            if len(idxs.shape) == 0:\n",
    "                idxs = idxs.view(1)\n",
    "            # Compute the subnet as the sum of scaled stacked masks for tasks with \n",
    "            # non-zero alpha weights\n",
    "            subnet = (\n",
    "                alpha_weights[idxs] * self.stacked[: self.num_tasks_learned][idxs]\n",
    "            ).sum(dim=0)\n",
    "        else:\n",
    "            # For single task, get subnet using GetSubnet class\n",
    "            subnet = GetSubnet.apply(self.scores[self.task])\n",
    "        # Multiply the weight by the subnet\n",
    "        w = self.weight * subnet\n",
    "\n",
    "        # Apply a linear transformation to the input tensor x using the computed weight \n",
    "        # and bias\n",
    "        x = F.linear(x, w, self.bias)\n",
    "        return x\n",
    "\n",
    "    # Override the representation method to display custom class name and dimensions\n",
    "    def __repr__(self):\n",
    "        return f\"MultitaskMaskLinear({self.in_dims}, {self.out_dims})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e2584",
   "metadata": {},
   "source": [
    "## Multitask Fully Connected Network (Classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to create a fully connected multitask network\n",
    "class MultitaskFC(nn.Module):\n",
    "    def __init__(self, hidden_size, num_tasks):\n",
    "        # Initialise the parent class\n",
    "        super().__init__()\n",
    "        # Define the model as a sequence of layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Add a multitask masked linear layer with input size 784 and output size hidden_size\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinear(784, hidden_size, num_tasks=num_tasks, bias=False),\n",
    "            # Normalise the output of the previous layer across the feature dimension\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            # Apply the ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Add a multitask masked linear layer with input size 784 and output size hidden_size\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinear(hidden_size, hidden_size, num_tasks=num_tasks, bias=False),\n",
    "            # Normalise the output of the previous layer across the feature dimension\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            # Apply the ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Add a multitask masked linear layer with input size hidden_size and output size 100\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinear(hidden_size, 100, num_tasks=num_tasks, bias=False),\n",
    "        )\n",
    "        \n",
    "        # Initialise means for each task's batch normalisation layer as a dictionary\n",
    "        self.bn_means = {i: {} for i in range(num_tasks)}\n",
    "\n",
    "    # Method to retrieve the running means of batch normalisation layers for a specific task\n",
    "    def get_bn_means(self, task_id):\n",
    "        # Iterate through the layers of the model\n",
    "        for i, layer in enumerate(self.model):\n",
    "            # Check if the layer is a batch normalisation layer\n",
    "            if isinstance(layer, nn.BatchNorm1d):\n",
    "                # Store the running mean of the batch normalisation layer for the specific task\n",
    "                self.bn_means[task_id][i] = layer.running_mean.detach().clone()\n",
    "        # Return the running means for the specified task\n",
    "        return self.bn_means[task_id]\n",
    "    \n",
    "    def get_masks(self, layer_index):\n",
    "        # Retrieve the layer by index from the model\n",
    "        layer = self.model[layer_index]\n",
    "        # Ensure the layer is an instance of MultitaskMaskLinear\n",
    "        if isinstance(layer, MultitaskMaskLinear):\n",
    "            # Return the masks for the specific layer\n",
    "            return [GetSubnet.apply(score) for score in layer.scores]\n",
    "        else:\n",
    "            # Handle cases where the layer is not an instance of MultitaskMaskLinear\n",
    "            raise ValueError(f\"Layer at index {layer_index} is not an instance of MultitaskMaskLinear.\")\n",
    "\n",
    "    \n",
    "    # Forward pass method to compute the model's output\n",
    "    def forward(self, x):\n",
    "        # Flatten the input x along dimension 1 and pass it through the sequential model\n",
    "        return self.model(x.flatten(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6b831",
   "metadata": {},
   "source": [
    "## MultiTask Masked Linear Layer (Novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3989e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to create a multitask linear layer with masks\n",
    "class MultitaskMaskLinearV2(nn.Linear):\n",
    "    def __init__(self, *args, num_tasks=1, **kwargs):\n",
    "        # Initialise the parent class with the provided arguments and keyword arguments\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Store the number of tasks\n",
    "        self.num_tasks = num_tasks\n",
    "        # Create a list of parameters for the scores, initialised with the mask_init \n",
    "        # function for each task\n",
    "        self.scores = nn.ParameterList(\n",
    "            [nn.Parameter(mask_init(self)) for _ in range(num_tasks)]\n",
    "        )\n",
    "        \n",
    "        # Disable gradients for the weight tensor to keep the weights untrained\n",
    "        self.weight.requires_grad = False\n",
    "        # Adjust the weights using the signed_constant function\n",
    "        signed_constant(self)\n",
    "\n",
    "    # Define a method to cache the subnet masks with no gradient tracking\n",
    "    @torch.no_grad()\n",
    "    def cache_masks(self):\n",
    "        # Register a buffer for the stacked masks\n",
    "        self.register_buffer(\n",
    "            \"stacked\",\n",
    "            # Stack the subnets for all tasks\n",
    "            torch.stack(\n",
    "                [GetSubnet.apply(self.scores[j]) for j in range(self.num_tasks)]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Override the forward pass method of the parent class\n",
    "    def forward(self, x):\n",
    "        # If task index is less than 0, perform a superimposed forward pass\n",
    "        if self.task < 0:\n",
    "            alpha_weights = self.alphas[: self.num_tasks_learned]\n",
    "            # Create a binary index mask for tasks with non-zero alpha_weights\n",
    "            idxs = (alpha_weights > 0).squeeze().view(self.num_tasks_learned)\n",
    "            if len(idxs.shape) == 0:\n",
    "                idxs = idxs.view(1)\n",
    "            # Compute the subnet as the sum of scaled stacked masks for tasks with \n",
    "            # non-zero alpha_weights\n",
    "            subnet = (\n",
    "                alpha_weights[idxs] * self.stacked[: self.num_tasks_learned][idxs]\n",
    "            ).sum(dim=0)\n",
    "        else:\n",
    "            # For single task, get subnet using GetSubnet class\n",
    "            subnet = GetSubnet.apply(self.scores[self.task])\n",
    "        # Multiply the weight by the subnet\n",
    "        w = self.weight * subnet\n",
    "\n",
    "        # Apply a linear transformation to the input tensor x using the computed weight \n",
    "        # and bias\n",
    "        x = F.linear(x, w, self.bias)\n",
    "        return x\n",
    "\n",
    "    # Override the representation method to display custom class name and dimensions\n",
    "    def __repr__(self):\n",
    "        return f\"MultitaskMaskLinearV2({self.in_dims}, {self.out_dims})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21ab49",
   "metadata": {},
   "source": [
    "## Multitask Fully Connected Network (Novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c51690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a subclass to create a fully connected multitask network\n",
    "class MultitaskFCV2(nn.Module):\n",
    "    def __init__(self, hidden_size, num_tasks):\n",
    "        # Initialise the parent class\n",
    "        super().__init__()\n",
    "        # Define the model as a sequence of layers\n",
    "        self.model = nn.Sequential(\n",
    "            # Add a multitask masked linear layer with input size 784 and output size 'hidden_size'\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinearV2(784, hidden_size, num_tasks=num_tasks, bias=False),\n",
    "            # Normalise the output of the previous layer across the feature dimension (dimension 1)\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            # Apply the ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Add another multitask masked linear layer with input size 'hidden_size' and output size 'hidden_size'\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinearV2(hidden_size, hidden_size, num_tasks=num_tasks, bias=False),\n",
    "            # Normalise the output of the previous layer across the feature dimension (dimension 1)\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            # Apply the ReLU activation function\n",
    "            nn.ReLU(),\n",
    "            # Add a multitask masked linear layer with input size 'hidden_size' and output size 100\n",
    "            # Each task has its own mask. Bias is disabled in this layer\n",
    "            MultitaskMaskLinearV2(hidden_size, 100, num_tasks=num_tasks, bias=False),\n",
    "        )\n",
    "        \n",
    "        # Initialise means for each task's batch normalisation layer as a dictionary\n",
    "        self.bn_means = {i: None for i in range(num_tasks)}\n",
    "        # Initialise current_task attribute, to identify the current task being processed\n",
    "        self.current_task = None\n",
    "            \n",
    "            \n",
    "    def set_alphas(self, alphas_per_layer, verbose=True):\n",
    "        # Iterate over the dictionary containing the alphas for each layer\n",
    "        for layer_index, alphas in alphas_per_layer.items():\n",
    "            # Iterate over named modules in the model to find MultitaskMaskLinearV2 instances\n",
    "            for n, m in self.named_modules():\n",
    "                # Check if the module is of type MultitaskMaskLinearV2 and if the current layer index matches\n",
    "                if isinstance(m, MultitaskMaskLinearV2) and int(n.split('.')[1]) in [0, 3]:\n",
    "                    # If verbose is True, print information about the alpha setting\n",
    "                    if verbose:\n",
    "                        print(f\"=> Setting alphas for {n}\")\n",
    "                    # Set the alphas value for the current module\n",
    "                    m.alphas = alphas\n",
    "\n",
    "\n",
    "    # Override the forward pass method of the parent class\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor x starting from the first dimension\n",
    "        x = x.flatten(1)\n",
    "        # Loop through each layer of the model\n",
    "        for i, layer in enumerate(self.model):\n",
    "            # Apply the layer on the input\n",
    "            x = layer(x)\n",
    "        return x # Return the processed tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfbb84",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "* https://github.com/pytorch\n",
    "* https://github.com/RAIVNLab/supsup\n",
    "* https://github.com/allenai/hidden-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
